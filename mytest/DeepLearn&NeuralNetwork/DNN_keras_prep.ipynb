{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 노트북 실행 결과를 동일하게 유지하기 위해\n",
    "np.random.seed(42)\n",
    "\n",
    "# 깔끔한 그래프 출력을 위해\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# 그림을 저장할 위치\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"그림 저장:\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "tf.config.experimental.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 심층 신경망 훈련하기 - DNN\n",
    "\n",
    "* 까다로운 그레디언트 소실 또는 폭주 문제에 직면하는 경우. \n",
    "  \n",
    "  심층 신경망의 아래쪽으로 갈수록 그레디언트가 점점 더 작아지거나 커지는 현상.\n",
    "\n",
    "* 대규모 신경망을 위한 훈련데이터가 충분치 않거나 레이블을 만드는 작업에 비용이 너무 많이 들 때.\n",
    "\n",
    "* 수백만 개의 파라미터를 가진 모델은 훈련세트에 과대적합될 위험이 매우 큼. \n",
    "  \n",
    "  특히 훈련샘플이 충분치 않거나 잡음이 많은 경우.\n",
    "\n",
    "|\n",
    "\n",
    "이번장에선 위의 문제들을 해결하고 전이 학습과 비지도 사전훈련을 알아보고, 대규모 모델의 최적화를\n",
    "\n",
    "알아볼 예정. 대규모 신경망 규제 기법 중 몇가지를 살펴볼 예정. -> 드디어 딥러닝에 도착."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그레디언트 소실과 폭주 문제\n",
    "\n",
    "* 그래디언트 소실(Vanishing Gradient): 알고리즘이 하위층으로 진행될수록 그래디언트가 점점 작아지는 경우\n",
    "\n",
    "* 그래디언트 폭주(Exploding Gradient): 그래디언트가 점점 커져서 여러 층이 비정상적으로 큰 가중치를 갱신되어 알고리즘이 발산하는 경우\n",
    "\n",
    "* 로지스틱 활성화 함수의 경우 입력이 커지면 0이나 1로 수렴해서 기울기가 0에 가까워짐\n",
    "\n",
    "* 역전파가 될 때 사실상 신경망으로 전파할 그래디언트가 거의 없고 조금 있는 그래디언트는 최상위층에서부터 역전파가 진행되면서 점차 약해져서 실제로 아래쪽 층에는 아무것도 도달하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그림 저장: [11-1] sigmoid_saturation_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"[11-1] sigmoid_saturation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 글로럿과 He 초기화\n",
    "\n",
    "* 글로럿과 벤지오가 불안정한 그래디언트 문제를 완화하는 방법 제안\n",
    "* 각 층의 출력에 대한 분산이 입력에 대한 분산과 같아야 한다고 주장\n",
    "* 역방향에서 층을 통과하기 전과 후의 그래디언트 분산이 동일\n",
    "* $fan_{avg}=(fan_{in}+fan_{out})$ 층의 입력 개수는 팬인 출력개수는 팬아웃\n",
    "\n",
    "|\n",
    "\n",
    "**식 11-1 글로럿 초기화(로지스틱 활성화 함수를 쓸 때)**\n",
    "\n",
    "$\n",
    "\\text{평균이 0이고 분산이} \\sigma^2=\\dfrac{1}{fan_{avg}} \\text{인 정규분포}\n",
    "$\n",
    "$\n",
    "\\text{또는} r =\\sqrt{\\dfrac{3}{fan_{avg}}}\\text{일 때} -r\\text{과} +r \\text{사이의 균등분포}\n",
    "$\n",
    "\n",
    "|\n",
    "\n",
    "[식 11-1에서] $fan_{avg}$를 fan_{in}으로 바꾸면 르쿤 초기화가 됨.\n",
    "\n",
    "글로럿 초기화를 하면 훈련 속도가 증가\n",
    "\n",
    "---\n",
    "\n",
    "초기화 전략\t| 활성화 함수 | $\\sigma^2$(정규분포)\n",
    "\n",
    "글로럿      | 활성화 함수 없음, tanh, logstic, softmax | $\\dfrac{1}{fan_{avg}}$\n",
    "\n",
    "He         | ReLU 함수와 그 변종들 | $\\dfrac{2}{fan_{in}}$\n",
    "\n",
    "르쿤        | SELU | $\\dfrac{1}{fan_{in}}$\n",
    "\n",
    "---\n",
    "\n",
    "케라스는 기본적으로 균등 분포의 글로럿 초기화를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같이 층을 만들 때 kernel_initializer=\"he_normal\"이나 kernel_initializer=\"he_uniform\"로\n",
    "\n",
    "바꾸어 He 초기화를 사용할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x21469169480>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$fan_{in}$대신 $fan_{avg}$기반의 균등분포 He 초기화를 사용하고 싶다면 \n",
    "\n",
    "다음과 같이 VarianceScaling 사용할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x21469169d80>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "                                          distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수렴하지 않는 활성화 함수\n",
    "\n",
    "활성화 함수를 잘못 선택하면 자칫 그레디언트의 소실이나 폭주로 이어질 수 있음.\n",
    "\n",
    "특히 ReLU 함수는 특정 양숫값에 수렴하지 않는다는 큰 장점이 있음(계산도 빠름).\n",
    "\n",
    "안타깝게도 ReLU 함수가 완벽하지 않아 죽은 ReLU 로 알려진 문제가 있음.\n",
    "\n",
    "---\n",
    "\n",
    "-> 훈련하는 동안 일부 뉴런이 0 이외의 값을 출력하지 않는다는 의미에서 죽었다고 함.\n",
    "\n",
    "ex) 특히 큰 학습률을 사용하면 신경망의 뉴런 절반이 죽어있기도 함. \n",
    "\n",
    "뉴런의 가중치가 바뀌어 훈련세트에 있는 모든 샘플에 대해 입력의 가중치 합이 음수가 되면 뉴런이 죽음.\n",
    "\n",
    "가중치 합이 음수면 ReLU함수의 그레디언트가 0이 되므로 경사하강법이 더는 작동 X\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyReLU\n",
    "\n",
    "ReLU의 뉴런이 죽는 현상을 해결하기 위해 나온 함수\n",
    "\n",
    "$\n",
    "LeakyReLU_\\alpha(x)=max(ax, x)$\n",
    "\n",
    "* 하이퍼파라미터 $\\alpha$가 이 함수가 새는(leaky) 정도를 결정한다. 새는 정도란 $x<0$\n",
    "  일 때 이 함수의 기울기이며, 일반적으로 0.01로 설정\n",
    "\n",
    "* Bing Xu et al., “Empirical Evaluation of Rectified Activations in Convolutional Network” \n",
    "  논문에 따르면 LeakyReLU가 ReLU보다 항상 성능이 높은 것으로 나온다.\n",
    "\n",
    "* $\\alpha=$ 0.2로 하는 것이 0.01보다 더 나은 성능을 내는 것으로 보임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그림 저장: [11-2] leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"[11-2] leaky_relu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeakyReLU를 사용해 패션 MNIST에서 신경망을 훈련해 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 1.2819 - accuracy: 0.6229 - val_loss: 0.8886 - val_accuracy: 0.7160\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.7955 - accuracy: 0.7361 - val_loss: 0.7130 - val_accuracy: 0.7656\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.6816 - accuracy: 0.7721 - val_loss: 0.6427 - val_accuracy: 0.7898\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.6217 - accuracy: 0.7944 - val_loss: 0.5900 - val_accuracy: 0.8066\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5832 - accuracy: 0.8075 - val_loss: 0.5582 - val_accuracy: 0.8202\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5553 - accuracy: 0.8157 - val_loss: 0.5350 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5338 - accuracy: 0.8225 - val_loss: 0.5157 - val_accuracy: 0.8304\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.5173 - accuracy: 0.8272 - val_loss: 0.5079 - val_accuracy: 0.8286\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5040 - accuracy: 0.8289 - val_loss: 0.4895 - val_accuracy: 0.8390\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4924 - accuracy: 0.8321 - val_loss: 0.4817 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RReLU (Randomized leaky ReLU)\n",
    "* $\\alpha=$ 를 무작위로 선택하고 테스트 시에는 평균을 사용하는 방법\n",
    "* 훈련 세트의 과대적함 위험을 줄이는 규제의 역할을 하는 것처럼 보임\n",
    "\n",
    "#### PReLU (Parametric leaky ReLU)\n",
    "* $\\alpha=$ 가 훈련하는 동안 학습되는 방법.(즉, 하이퍼파라미터가 아니고 다른 모델 파라미터와 \n",
    "  마찬가지로 역전파에 의해 변경)\n",
    "* 대규모 이미지 데이터셋에서는 ReLU보다 성능이 크게 앞섰지만, 소규모 데이터셋에서는 훈련 세트에 \n",
    "  과대적합될 위험이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PReLU를 테스트해 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 1.3461 - accuracy: 0.6209 - val_loss: 0.9255 - val_accuracy: 0.7186\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.8197 - accuracy: 0.7355 - val_loss: 0.7305 - val_accuracy: 0.7630\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.6966 - accuracy: 0.7693 - val_loss: 0.6565 - val_accuracy: 0.7880\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.6331 - accuracy: 0.7909 - val_loss: 0.6003 - val_accuracy: 0.8046\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5917 - accuracy: 0.8057 - val_loss: 0.5656 - val_accuracy: 0.8178\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5618 - accuracy: 0.8135 - val_loss: 0.5406 - val_accuracy: 0.8240\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5390 - accuracy: 0.8205 - val_loss: 0.5196 - val_accuracy: 0.8316\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5213 - accuracy: 0.8257 - val_loss: 0.5113 - val_accuracy: 0.8316\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5070 - accuracy: 0.8287 - val_loss: 0.4916 - val_accuracy: 0.8380\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4945 - accuracy: 0.8315 - val_loss: 0.4826 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELU (Exponential Linear Unit)\n",
    "\n",
    "* 다른 모든 ReLU 변종의 성능을 앞지렀다.\n",
    "* 훈련 시간이 줄고 신경망의 테스트 세트 성능도 더 높았다.\n",
    "\n",
    " \n",
    "\n",
    "**Equation 11-2: ELU activation function**\n",
    "\n",
    "$\\operatorname{ELU}_\\alpha (z) =\\begin{cases}\n",
    "\\alpha(\\exp(z) - 1) & \\text{if } z < 0\\\\\n",
    "z & if z \\ge 0\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그림 저장: [11-3] elu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"[11-3] elu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* x<0일 때 음숫값이 들어오므로 활성화 함수의 평균 출력이 0에 더 가깝다. 이는 그래디언트 소실 문제를 완화. 하이퍼파라미터 α는 x가 큰 음숫값일 때 ELU가 수렴할 값을 정의\n",
    "\n",
    "* x<0이어도 그래디언트가 0이 아니므로 죽은 뉴런을 만들지 않는다.\n",
    "\n",
    "* α=1이면 이 함수는 x=0에서 급격히 변동하지 않으므로 x=0을 포함해 모든 구간에서 매끄러워 경사 하강법의 속도를 높여준다\n",
    "\n",
    "* 단점: ReLU나 그 변종들보다 계산이 느리다.(훈련하는 동안에는 수렴 속도가 빨라서 느린 계산이 상쇄되지만 테스트 시에는 느릴 것)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELU\n",
    "\n",
    "* ELU 활성화 함수의 변종\n",
    "* 완전 연결 층만 쌓아서 신경망을 만들고 모든 은닉층이 SELU 활성화 함수를 사용하면 네트워크가 자기 정규화(self-normalized) 된다고 저자는 주장\n",
    "* 훈련하는 동안 각 층의 출력이 평균 0과 표준편차 1을 유지하는 경향(그래디언트 소실과 폭주 문제를 막아준다.)\n",
    "* 다른 활성화 함수보다 뛰어난 성능을 종종 보이지만 자기 정규화가 일어나기 위한 몇가지 조건이 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Günter Klambauer, Thomas Unterthiner, Andreas Mayr는 2017년 한 [훌륭한 논문](https://arxiv.org/pdf/1706.02515.pdf)에서 SELU 활성화 함수를 소개했습니다. 훈련하는 동안 완전 연결 층만 쌓아서 신경망을 만들고 SELU 활성화 함수와 LeCun 초기화를 사용한다면 자기 정규화됩니다. 각 층의 출력이 평균과\n",
    "표준편차를 보존하는 경향이 있습니다. 이는 그레이디언트 소실과 폭주 문제를 막아줍니다. 그 결과로 SELU 활성화 함수는 이런 종류의 네트워크(특히 아주 깊은 네트워크)에서 다른 활성화 함수보다 뛰어난 성능을 종종 냅니다. 따라서 꼭 시도해 봐야 합니다. 하지만 SELU 활성화 함수의 자기 정규화 특징은 쉽게 깨집니다. ℓ<sub>1</sub>나 ℓ<sub>2</sub> 정규화, 드롭아웃, 맥스 노름, 스킵 연결이나 시퀀셜하지 않은 다른 토폴로지를 사용할 수 없습니다(즉 순환 신경망은 자기 정규화되지 않습니다). 하지만 실전에서 시퀀셜 CNN과 잘 동작합니다. 자기 정규화가 깨지면 SELU가 다른 활성화 함수보다 더 나은 성능을 내지 않을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha와 scale은 평균 0과 표준 편차 1로 자기 정규화합니다\n",
    "# (논문에 있는 식 14 참조):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그림 저장: selu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"selu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 입력 특성이 반드시 표준화(평균 0, 표준편차 1)되어야 한다.\n",
    "\n",
    "2) 모든 은닉층의 가중치는 르쿤 정규분포 초기화로 초기화되어야 한다. 케라스에서는 kernel_initializer=’lecun_normal’로 설정\n",
    "\n",
    "3) 네트워크는 일렬로 쌓은 층으로 구성되어야 한다. 순환 신경망이나 스킵 연결과 같은 순차적이지 않은 구조에서 사용하면 자기 정규화되는 것을 보장하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "쉽게 SELU를 사용할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x2147b7a2b00>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"selu\",\n",
    "                   kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 83s 48ms/step - loss: 0.5354 - accuracy: 0.8102 - val_loss: 0.4973 - val_accuracy: 0.8254\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 86s 50ms/step - loss: 0.4897 - accuracy: 0.8288 - val_loss: 0.4654 - val_accuracy: 0.8410\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 88s 51ms/step - loss: 0.4765 - accuracy: 0.8324 - val_loss: 0.4615 - val_accuracy: 0.8402\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 84s 49ms/step - loss: 0.4349 - accuracy: 0.8474 - val_loss: 0.4414 - val_accuracy: 0.8448\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 86s 50ms/step - loss: 0.4140 - accuracy: 0.8554 - val_loss: 0.4283 - val_accuracy: 0.8580\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좋지 않군요. 그레이디언트 폭주나 소실 문제가 발생한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 일반적으로 SELU > ELU > LeakyReLU(그리고 변종들) > ReLU > tanh > sigmoid 순\n",
    "* 네트워크가 자기 정규화되지 못하는 구조라면 SELU 보단 ELU\n",
    "* 실행 속도가 중요하다면 LeakyReLU(하이퍼파라미터를 더 추가하고 싶지 않다면 케라스에서 사용하는 기본값 \n",
    "$\\alpha$ 사용)\n",
    "* 시간과 컴퓨팅 파워가 충분하다면 교차 검증을 사용해 여러 활성화 함수를 평가\n",
    "* 신경망이 과대적합되었다면 RReLU\n",
    "* 훈련세트가 아주 크다면 PReLU\n",
    "* ReLU가 가장 널리 사용되는 활성화 함수이므로 많은 라이브러리와 하드웨어 가속기들이 ReLU에 특화되어 최적화. 따라서 속도가 중요하다면 ReLU가 가장 좋은 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 정규화 (BN)\n",
    "\n",
    "* 훈련하는 동안의 그래디언트 소실이나 폭주문제를 해결하기 위한 방법\n",
    "* 세르게이 이오페, 치리슈티언 세게지가 제안\n",
    "* 각 층에서 활성화 함수를 통과하기 전이나 후에 모델에 연산을 하나 추가\n",
    "* 입력을 원점에 맞추고 정규화한 다음, 각 층에서 두 개의 새로운 파라미터로 결괏값의 스케일을 조정하고 이동\n",
    "* 신경망의 첫 번째 층으로 배치 정규화를 추가하면 훈련세트를 표준화할 필요가 없다.\n",
    "\n",
    "---\n",
    "\n",
    "**Equation 11-3: Batch Normalization algorithm**\n",
    "\n",
    "$\\begin{split}\n",
    "1.\\quad & \\mathbf{\\mu}_B = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{\\mathbf{x}^{(i)}}\\\\\n",
    "2.\\quad & {\\mathbf{\\sigma}_B}^2 = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{(\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B)^2}\\\\\n",
    "3.\\quad & \\hat{\\mathbf{x}}^{(i)} = \\dfrac{\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B}{\\sqrt{{\\mathbf{\\sigma}_B}^2 + \\epsilon}}\\\\\n",
    "4.\\quad & \\mathbf{z}^{(i)} = \\gamma \\hat{\\mathbf{x}}^{(i)} + \\beta\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mu_B$는 미니배치 B에 대해 평가한 입력의 평균 벡터(입력마다 하나의 평균)\n",
    "\n",
    "$\\sigma_B$도 미니배치에 대해 평가한 입력의 표준편차 벡터\n",
    "\n",
    "$m_B$는 미니배치에 있는 샘플 수\n",
    "\n",
    "$\\hat{x^{(i)}}$는 평균이 0이고 정규화된 샘플 i의 입력\n",
    "\n",
    "$\\gamma$는 층의 출력 스케일 파라미터 벡터(입력마다 하나의 스케일 파라미터가 존재)\n",
    "\n",
    "⊗는 원소별 곱셈(각 입력은 해당되는 출력 스케일 파라미터와 곱해진다.)\n",
    "\n",
    "$\\beta$는 층의 출력 이동(오프셋) 파라미터 벡더(입력마다 하나의 스케일 파라미터 존재). 각 입력은 해당 파라미터만큼 이동\n",
    "\n",
    "$\\theta$은 분모가 0이 되는 것을 막기 위한 작은 숫자(전형적으로 $10^{-5}$). 안전을 위한 항(smoothing term) 이라고 함.\n",
    "\n",
    "* $z^{(i)}$ 는 배치 정규화 연산의 출력. 즉 입력의 스케일을 조정하고 이동시킨 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 테스트 시에는 훈련이 끝난 후 전체 훈련 세트를 신경망에 통과시켜 배치 정규화 층의 각 입력에 대한 평균과 표준편차를 계산하여 예측할 때 배치 입력 평균과 표준 편차로 이 ‘최종’ 입력 평균과 표준편차를 대신 사용하는 방법이 있지만 대부분 층의 입력 평균과 표준편차의 이동 평균을 사용해 훈련하는 동안 최종 통계를 추정\n",
    "* 케라스의 BatchNormalization 층은 이를 자동으로 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\gamma$(출력 스케일 벡터)와 $\\beta$(출력 이동 벡터)는 일반적인 역전파를 통해 학습\n",
    "\n",
    "2. $\\mu$(최종 입력 평균 벡터)와 $\\sigma$(최종 입력 표준편차 벡터)는 지수 이동 평균을 사용하여 추정\n",
    "\n",
    "3. $\\mu$와 $\\sigma$는 훈련하는 동안 추정되지만 훈련이 끝난 후에 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이미지넷 분류 작업에서 큰 성과\n",
    "* 그래디언트 소실 문제가 크게 감소하여 tanh나 로지스틱 함수와 같은 수렴성을 가진 활성화 함수와도 사용 가능\n",
    "* 가중치 초기화에 네트워크가 훨씬 덜 민감\n",
    "* 규제와도 같은 역할을 하여 다른 규제 기법의 필요성 감소\n",
    "* 단점: 모델의 복잡도가 커짐, 실행 시간 증가(대신 이전 층의 가중치를 바꾸어 이전 층과 배치 정규화 층이 합쳐진 결과를 내어 실행 시간을 단축시킬 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**케라스로 배치 정규화 구현하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_12 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_345 (Dense)           (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_346 (Dense)           (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_347 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 볼 수 있듯이 배치 정규화 층은 입력마다 4개의 파라미터를 추가함.\n",
    "\n",
    "(ex. 첫번째 배치 정규화 층은 4*784=3136개의 파라미터가 있음)\n",
    "\n",
    "마지막 2개의 파라미터 $\\mu$와 $\\sigma$는 이동평균. 이 파라미터는 역전파로 학습되지 않기 때문에\n",
    "\n",
    "케라스는 'Non-trainable' 파라미터로 분류함(3136+1200+400을 2로 나누면 2368이 나옴.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 19s 10ms/step - loss: 0.8750 - accuracy: 0.7124 - val_loss: 0.5525 - val_accuracy: 0.8228\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 0.5753 - accuracy: 0.8030 - val_loss: 0.4724 - val_accuracy: 0.8470\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.5189 - accuracy: 0.8203 - val_loss: 0.4375 - val_accuracy: 0.8550\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.4827 - accuracy: 0.8323 - val_loss: 0.4152 - val_accuracy: 0.8606\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.4564 - accuracy: 0.8408 - val_loss: 0.3997 - val_accuracy: 0.8640\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.4397 - accuracy: 0.8473 - val_loss: 0.3866 - val_accuracy: 0.8702\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.4241 - accuracy: 0.8514 - val_loss: 0.3763 - val_accuracy: 0.8706\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.4143 - accuracy: 0.8540 - val_loss: 0.3712 - val_accuracy: 0.8740\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 20s 12ms/step - loss: 0.4024 - accuracy: 0.8579 - val_loss: 0.3631 - val_accuracy: 0.8756\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.3915 - accuracy: 0.8622 - val_loss: 0.3571 - val_accuracy: 0.8752\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이따금 활성화 함수전에 BN을 적용해도 잘 동작합니다(여기에는 논란의 여지가 있습니다). 또한 `BatchNormalization` 층 이전의 층은 편향을 위한 항이 필요 없습니다. `BatchNormalization` 층이 이를 무효화하기 때문입니다. 따라서 필요 없는 파라미터이므로 `use_bias=False`를 지정하여 층을 만들 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 1.0317 - accuracy: 0.6756 - val_loss: 0.6767 - val_accuracy: 0.7810\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.6790 - accuracy: 0.7793 - val_loss: 0.5566 - val_accuracy: 0.8182\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.5960 - accuracy: 0.8037 - val_loss: 0.5007 - val_accuracy: 0.8358\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.5447 - accuracy: 0.8191 - val_loss: 0.4666 - val_accuracy: 0.8448\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 0.5109 - accuracy: 0.8278 - val_loss: 0.4434 - val_accuracy: 0.8536\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 0.4898 - accuracy: 0.8337 - val_loss: 0.4263 - val_accuracy: 0.8544\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.4712 - accuracy: 0.8397 - val_loss: 0.4130 - val_accuracy: 0.8568\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.4560 - accuracy: 0.8438 - val_loss: 0.4035 - val_accuracy: 0.8608\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.4441 - accuracy: 0.8475 - val_loss: 0.3943 - val_accuracy: 0.8640\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 16s 10ms/step - loss: 0.4332 - accuracy: 0.8505 - val_loss: 0.3875 - val_accuracy: 0.8660\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그레디언트 클리핑\n",
    "\n",
    "* 역전파될 때 일정 임곗값을 넘어서지 못하게 그래디언트를 자르는 방법\n",
    "* 케라스에서 구현하려면 옵티마이저를 만들 때 clipvalue와 clipnorm 매개변수를 지정\n",
    "\n",
    "모든 케라스 옵티마이저는 `clipnorm`이나 `clipvalue` 매개변수를 지원합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전훈련된 층 재사용 - 전이학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 케라스를 사용한 전이 학습\n",
    "\n",
    "패션 MNIST 훈련 세트를 두 개로 나누어 보죠:\n",
    "* `X_train_A`: 샌달과 셔츠(클래스 5와 6)을 제외한 모든 이미지\n",
    "* `X_train_B`: 샌달과 셔츠 이미지 중 처음 200개만 가진 작은 훈련 세트\n",
    "\n",
    "검증 세트와 테스트 세트도 이렇게 나눕니다. 하지만 이미지 개수는 제한하지 않습니다.\n",
    "\n",
    "A 세트(8개의 클래스를 가진 분류 문제)에서 모델을 훈련하고 이를 재사용하여 B 세트(이진 분류)를 해결해 보겠습니다. A 작업에서 B 작업으로 약간의 지식이 전달되기를 기대합니다. 왜냐하면 A 세트의 클래스(스니커즈, 앵클 부츠, 코트, 티셔츠 등)가 B 세트에 있는 클래스(샌달과 셔츠)와 조금 비슷하기 때문입니다. 하지만 `Dense` 층을 사용하기 때문에 동일한 위치에 나타난 패턴만 재사용할 수 있습니다(반대로 합성곱 층은 훨씬 많은 정보를 전송합니다. 학습한 패턴을 이미지의 어느 위치에서나 감지할 수 있기 때문입니다. CNN 장에서 자세히 알아 보겠습니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43986, 28, 28)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 28, 28)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 10s 6ms/step - loss: 0.5926 - accuracy: 0.8103 - val_loss: 0.3890 - val_accuracy: 0.8677\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 8s 6ms/step - loss: 0.3523 - accuracy: 0.8786 - val_loss: 0.3288 - val_accuracy: 0.8822\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 8s 6ms/step - loss: 0.3170 - accuracy: 0.8894 - val_loss: 0.3013 - val_accuracy: 0.8989\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 8s 6ms/step - loss: 0.2973 - accuracy: 0.8975 - val_loss: 0.2894 - val_accuracy: 0.9016\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 8s 6ms/step - loss: 0.2835 - accuracy: 0.9021 - val_loss: 0.2775 - val_accuracy: 0.9066\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 8s 6ms/step - loss: 0.2729 - accuracy: 0.9061 - val_loss: 0.2735 - val_accuracy: 0.9068\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 9s 6ms/step - loss: 0.2641 - accuracy: 0.9092 - val_loss: 0.2719 - val_accuracy: 0.9083\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 9s 6ms/step - loss: 0.2573 - accuracy: 0.9124 - val_loss: 0.2590 - val_accuracy: 0.9136\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 8s 6ms/step - loss: 0.2518 - accuracy: 0.9137 - val_loss: 0.2562 - val_accuracy: 0.9148\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 10s 7ms/step - loss: 0.2469 - accuracy: 0.9154 - val_loss: 0.2542 - val_accuracy: 0.9153\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 9s 6ms/step - loss: 0.2422 - accuracy: 0.9177 - val_loss: 0.2495 - val_accuracy: 0.9155\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 8s 6ms/step - loss: 0.2382 - accuracy: 0.9185 - val_loss: 0.2507 - val_accuracy: 0.9126\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 9s 6ms/step - loss: 0.2350 - accuracy: 0.9199 - val_loss: 0.2444 - val_accuracy: 0.9155\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 9s 7ms/step - loss: 0.2315 - accuracy: 0.9212 - val_loss: 0.2415 - val_accuracy: 0.9175\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 9s 7ms/step - loss: 0.2287 - accuracy: 0.9212 - val_loss: 0.2445 - val_accuracy: 0.9185\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 9s 6ms/step - loss: 0.2254 - accuracy: 0.9223 - val_loss: 0.2386 - val_accuracy: 0.9195\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 9s 6ms/step - loss: 0.2230 - accuracy: 0.9231 - val_loss: 0.2408 - val_accuracy: 0.9173\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 9s 6ms/step - loss: 0.2201 - accuracy: 0.9245 - val_loss: 0.2423 - val_accuracy: 0.9150\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 9s 7ms/step - loss: 0.2178 - accuracy: 0.9254 - val_loss: 0.2329 - val_accuracy: 0.9203\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 9s 7ms/step - loss: 0.2156 - accuracy: 0.9259 - val_loss: 0.2331 - val_accuracy: 0.9205\n"
     ]
    }
   ],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")\n",
    "\n",
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 63ms/step - loss: 0.9573 - accuracy: 0.4650 - val_loss: 0.6314 - val_accuracy: 0.6004\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.5692 - accuracy: 0.7450 - val_loss: 0.4784 - val_accuracy: 0.8529\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.4503 - accuracy: 0.8650 - val_loss: 0.4102 - val_accuracy: 0.8945\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.3879 - accuracy: 0.8950 - val_loss: 0.3647 - val_accuracy: 0.9178\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.3435 - accuracy: 0.9250 - val_loss: 0.3300 - val_accuracy: 0.9320\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.3081 - accuracy: 0.9300 - val_loss: 0.3019 - val_accuracy: 0.9402\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.2800 - accuracy: 0.9350 - val_loss: 0.2804 - val_accuracy: 0.9422\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.2564 - accuracy: 0.9450 - val_loss: 0.2606 - val_accuracy: 0.9473\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.2362 - accuracy: 0.9550 - val_loss: 0.2428 - val_accuracy: 0.9523\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.2188 - accuracy: 0.9600 - val_loss: 0.2281 - val_accuracy: 0.9544\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.2036 - accuracy: 0.9700 - val_loss: 0.2150 - val_accuracy: 0.9584\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.1898 - accuracy: 0.9700 - val_loss: 0.2036 - val_accuracy: 0.9584\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.1773 - accuracy: 0.9750 - val_loss: 0.1931 - val_accuracy: 0.9615\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.1668 - accuracy: 0.9800 - val_loss: 0.1838 - val_accuracy: 0.9635\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.1570 - accuracy: 0.9900 - val_loss: 0.1746 - val_accuracy: 0.9686\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.1481 - accuracy: 0.9900 - val_loss: 0.1674 - val_accuracy: 0.9686\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.1406 - accuracy: 0.9900 - val_loss: 0.1604 - val_accuracy: 0.9706\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.1334 - accuracy: 0.9900 - val_loss: 0.1539 - val_accuracy: 0.9706\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.1268 - accuracy: 0.9900 - val_loss: 0.1482 - val_accuracy: 0.9716\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.1208 - accuracy: 0.9900 - val_loss: 0.1431 - val_accuracy: 0.9716\n"
     ]
    }
   ],
   "source": [
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_15 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_357 (Dense)           (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_358 (Dense)           (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_359 (Dense)           (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_360 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_361 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_362 (Dense)           (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_B_on_A`와 `model_A`는 층을 공유하기 때문에 하나를 훈련하면 두 모델이 업데이트됩니다. 이를 피하려면 `model_A`를 클론한 것을 사용해 `model_B_on_A`를 만들어야 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 작업 B를 위해 model_B_on_A를 훈련할 수 있음. 하지만 새로운 출력층이\n",
    "\n",
    "랜덤하게 초기화되어 있으므로 큰 오차를 만들 것. 따라서 큰 오차 그레디언트가 재사용된 가중치를\n",
    "\n",
    "망칠 수 있음. 이를 피하기 위해 처음 몇 번의 에포크 동안은 재사용된 층 동결하고 새로운 층에게\n",
    "\n",
    "적절한 학습할 시간을 주는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False     # 재사용된 층 동결을 위해 False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 1s 56ms/step - loss: 0.2674 - accuracy: 0.9350 - val_loss: 0.2814 - val_accuracy: 0.9249\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2576 - accuracy: 0.9400 - val_loss: 0.2716 - val_accuracy: 0.9300\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.2481 - accuracy: 0.9400 - val_loss: 0.2628 - val_accuracy: 0.9331\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.2394 - accuracy: 0.9400 - val_loss: 0.2545 - val_accuracy: 0.9361\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 56ms/step - loss: 0.2140 - accuracy: 0.9450 - val_loss: 0.2057 - val_accuracy: 0.9645\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.1709 - accuracy: 0.9550 - val_loss: 0.1729 - val_accuracy: 0.9716\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.1416 - accuracy: 0.9650 - val_loss: 0.1499 - val_accuracy: 0.9807\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.1204 - accuracy: 0.9800 - val_loss: 0.1332 - val_accuracy: 0.9817\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.1051 - accuracy: 0.9900 - val_loss: 0.1206 - val_accuracy: 0.9838\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.0933 - accuracy: 0.9950 - val_loss: 0.1106 - val_accuracy: 0.9858\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.0841 - accuracy: 0.9950 - val_loss: 0.1025 - val_accuracy: 0.9858\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0765 - accuracy: 0.9950 - val_loss: 0.0957 - val_accuracy: 0.9868\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.0701 - accuracy: 0.9950 - val_loss: 0.0895 - val_accuracy: 0.9868\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.0643 - accuracy: 0.9950 - val_loss: 0.0847 - val_accuracy: 0.9888\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0598 - accuracy: 0.9950 - val_loss: 0.0803 - val_accuracy: 0.9888\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0556 - accuracy: 1.0000 - val_loss: 0.0765 - val_accuracy: 0.9878\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0519 - accuracy: 1.0000 - val_loss: 0.0731 - val_accuracy: 0.9878\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0487 - accuracy: 1.0000 - val_loss: 0.0703 - val_accuracy: 0.9878\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.0678 - val_accuracy: 0.9878\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 0.0654 - val_accuracy: 0.9878\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True      # 재사용된 층의 동결 해제\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 점수는 어떤가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1408 - accuracy: 0.9705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1408407837152481, 0.9704999923706055]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0563 - accuracy: 0.9940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05628018453717232, 0.9940000176429749]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차율이 4.9배나 줄음!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.916666666666718"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100 - 97.05) / (100 - 99.40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "믿을 만한가? 사실 믿으면 안 됨. 속임수 존재! 왜 속임수를 썼을까?\n",
    "\n",
    "전이학습은 작은 완전 연결 네트워크에서는 잘 동작하지 않기 때문. 아마 작은 네트워크는 패턴 수를 적게\n",
    "\n",
    "학습하고 완전 연결 네트워크는 특정 패턴을 학습하기 때문일 것. \n",
    "\n",
    "전이학습은 좀 더 일반적인 특성을 감지하는 경향이 있는 심층합성곱 신경망에서 잘 동작. \n",
    "\n",
    "(14장에서 다룰 예정)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비지도 사전훈련\n",
    "\n",
    "더 많은 레이블된 훈련데이터를 모이기 어려울 때 사용하는 방법.\n",
    "\n",
    "레이블되지 않은 훈련데이터를 많이 모을 수 있다면 이를 사용해 \n",
    "\n",
    "오토인코더나 생성적 적대 신경망(17장 참조)과 같은 비지도 학습 모델을 훈련할 수 있음.\n",
    "\n",
    "그다음 오토인코더나 GAN 판별자의 하위층을 재사용하고 그 위에 새로운 작업에 맞는 출력층을 추가가능.\n",
    "\n",
    "그다음 지도 학습으로 (즉, 레이블된 훈련 샘플로) 최종 네트워크를 세밀하게 튜닝."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보조 작업에서 사전훈련\n",
    "\n",
    "레이블된 훈련데이터가 많지 않다면 마지막 선택사항은 레이블된 훈련데이터를 쉽게 얻거나 생성가능한\n",
    "\n",
    "보조 작업에서 첫번째 신경망을 훈련하는 것. 그리고 이 신경망의 하위층을 실제작업을 위해 재사용.\n",
    "\n",
    "첫번째 신경망의 하위층은 두번째 신경망에 재사용될 수 있는 특성 추출기를 학습하게 됨.\n",
    "\n",
    "|\n",
    "\n",
    "자연어 처리(NLP) 애플리케이션에서는 수백만 개의 텍스트 문서로 이루어진 코퍼스를\n",
    "\n",
    "다운로드하고 이 데이터에서 레이블된 데이터를 자동으로 생성할 수 있음. \n",
    "\n",
    "(15장에서 사전훈련 작업에 대해 더 알아보겠습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 고속 옵티마이저"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모멘텀 최적화\n",
    "\n",
    "표준적인 경사 하강법은 경사면을 따라 일정한 크기의 스텝으로 조금씩 내려가는 반면 모멘텀 최적화는 처음에는 느리게 출발하지만 종단속도에 도달할 때가지는 빠르게 가속.\n",
    "\n",
    "**Equation 11-4: Momentum algorithm**\n",
    "\n",
    "1. $\\mathbf{m} \\gets \\beta \\mathbf{m} - \\eta \\nabla_\\theta J(\\theta)$\n",
    "\n",
    "2. $\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} + \\mathbf{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모멘텀 최적화는 이전 그래디언트가 얼마였는지가 상당히 중요\n",
    "* 현재 그래디언트를 (학습률 $\\eta$를 곱한 후) 모멘텀 벡터 $m$에 더하고 \n",
    "  이 값을 빼는 방식으로 가중치를 갱신\n",
    "* 일종의 마찰저항을 표현하고 모멘텀이 너무 커지는 것을 막기 위해 모멘텀이라는 새로운 하이퍼파라미터 \n",
    "  $\\beta$ 사용(일반적인 모멘텀 값은 0.9)더 바르게 평편한 지역을 탈출하게 도움\n",
    "* 경사 하강법이 가파른 경사를 꽤 빠르게 내려가지만 좁고 긴 골짜기에서는 오랜 시간이 걸린다. \n",
    "  모멘텀 최적화는 골짜기를 따라 바닥(최적점)에 도달할 때가지 점점 더 빠르게 내려간다.\n",
    "* 배치 정규화를 사용하지 않는 심층 신경망에서 상위층은 종종 스케일이 매우 다른 입력을 받게 되는데 \n",
    "  이런 경우 사용하면 도움이 된다.\n",
    "* 또한 지역 최적점을 건너뛰도록 하는 데도 도움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네스테로프 가속 경사\n",
    "\n",
    "현재 위치가 $\\theta$가 아니라 모멘텀 방향으로 조금 앞선 $\\theta_\\beta m$ 에서 비용 함수의 그래디언트를 계산하는 것\n",
    "\n",
    "**Equation 11-5: Nesterov Accelerated Gradient algorithm**\n",
    "\n",
    "1. $\\mathbf{m} \\gets \\beta \\mathbf{m} - \\eta \\nabla_\\theta J(\\theta + \\beta m)$\n",
    "2. $\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} + \\mathbf{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 원래 위치에서의 그래디언트를 사용하는 것보다 그 방향으로 조금 더 나아가서 \n",
    "  측정한 그래디언트를 사용하는 것이 더 정확할 것\n",
    "* 진동을 감소시키고 수렴을 빠르게 만들어 준다.\n",
    "* 일반적으로 기본 모멘텀 최적화보다 훈련 속도가 빠르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "가장 가파른 차원을 따라 그래디언트 벡터의 스케일을 감소\n",
    "\n",
    "**Equation 11-6: AdaGrad algorithm**\n",
    "\n",
    "1. $ \\mathbf{s} \\gets \\mathbf{s} + \\nabla_\\theta J(\\boldsymbol{\\theta}) \\otimes \\nabla_\\theta J(\\boldsymbol{\\theta}) $\n",
    "2. $ \\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} - \\eta, \\nabla_\\theta J(\\boldsymbol{\\theta}) \\oslash {\\sqrt{\\mathbf{s} + \\epsilon}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 첫 번째 단계로 그래디언트 제곱을 벡터 $s$에 누적(각 원소 $s_i$마다 파라미터 $θ_i$ 에 대한 \n",
    "  비용 함수의 편미분을 제곱하여 누적)\n",
    "* 두 번째 단계로 그래디언트 벡터를 $\\sqrt{s+\\epsilon}$ 으로 나누어 스케일을 조정 \n",
    "  (⊘기호는 원소별 나눗셈을 나타내고 $\\epsilon$ 은 0으로 나누는 것을 막기 위한 값으로, \n",
    "  일반적으로 $10^{-10}$)\n",
    "* 이 알고리즘은 학습률을 감소시키지만 경사가 완만한 차원보다 가파른 차원에 대해 더 빠르게 감소하며 \n",
    "  이를 적응적 학습률(adaptive learning)이라고 부르며, 전역 최적점 방향으로 \n",
    "  더 곧장 가도록 갱신되는 데 도움\n",
    "* 학습률 하이퍼파라미터 $\\eta$를 덜 튜닝해도 되는 점이 또 하나의 장점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 간단한 2차 방정식 문제에 대해서는 잘 작동하지만 신경망을 훈련할 때 너무 일직 멈추는 경우가 있음.  \n",
    "  학습률이 너무 감소되어 전역 최적점에 도착하기전에 알고리즘이 멈춤.\n",
    "* 케라스에 Adagrad 옵티마이저가 있지만 심층 신경망에 사용하지 말 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "* 가장 최근 반복에서 비롯된 그래디언트만 누적함으로써 AdaGrad가 \n",
    "  전역 최적점에 도착하기 전에 알고리즘이 멈추는 문제를 해결\n",
    "* 지수 감소를 사용\n",
    "\n",
    "**Equation 11-7: RMSProp algorithm**\n",
    "\n",
    "1. $\\mathbf{s} \\gets \\beta \\mathbf{s} + (1 - \\beta ) \\nabla_\\theta J(\\boldsymbol{\\theta}) \\otimes \\nabla_\\theta J(\\boldsymbol{\\theta})$\n",
    "2. $\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} - \\eta \\, \\nabla_\\theta J(\\boldsymbol{\\theta}) \\oslash {\\sqrt{\\mathbf{s} + \\epsilon}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보통 감쇠율 $\\beta$ 는 0.9로 설정\n",
    "* 기본값이 잘 작동하는 경우가 많으므로 튜닝할 필요는 전혀 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* rho 매겨변수는 $\\beta$\n",
    "* AdaGrad보다 훨씬 더 성능이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam과 Nadam 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "* 적응적 모멘트 추정(adaptive moment estimation)을 의미\n",
    "* 모멘텀 최적화와 RMSProp의 아이디어를 합친 것\n",
    "* 모멘텀 최적화처럼 지난 그래디언트의 지수 감소 평균을 따르고 RMSProp처럼 \n",
    "  지난 그래디언트 제곱의 지수 감소된 평균을 따른다.\n",
    "\n",
    "**Equation 11-8: Adam algorithm**\n",
    "\n",
    "1. $\\mathbf{m} \\gets \\beta_1 \\mathbf{m} - (1 - \\beta_1) \\nabla_\\theta J(\\boldsymbol{\\theta})$\n",
    "2. $\\mathbf{s} \\gets \\beta_2 \\mathbf{s} + (1 - \\beta_2) \\nabla_\\theta J(\\boldsymbol{\\theta}) \\otimes \\nabla_\\theta J(\\boldsymbol{\\theta})$\n",
    "3. $\\hat{\\mathbf{m}} \\gets \\left(\\dfrac{\\mathbf{m}}{1 - {\\beta_1}^T}\\right)$\n",
    "4. $\\hat{\\mathbf{s}} \\gets \\left(\\dfrac{\\mathbf{s}}{1 - {\\beta_2}^T}\\right)$\n",
    "5. $\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} + \\eta \\, \\hat{\\mathbf{m}} \\oslash {\\sqrt{\\hat{\\mathbf{s}} + \\epsilon}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* t는 (1부터 시작하는) 반복 횟수\n",
    "* 단계 1,2,5는 모멘텀 최적화, RMSProp과 비슷\n",
    "* 단계 3,4에서 m,s는 0으로 초기화되기 때문에 훈련 초기에 0쪽으로 치우치게 된다. \n",
    "  그래서 이 두 단계가 훈련 초기에 m,s의 값을 증폭시키는 데 도움을 준다.\n",
    "* 반복 초기에는 m,s를 증폭시켜주지만 반복이 많이 진행되면 단계 3,4의 분모는 1에 가까워져 \n",
    "  거의 증폭되지 않는다.\n",
    "* 모멘텀 감쇠 하이퍼파라미터 $\\beta_1$은 보통 0.9로, 스케일 감쇠 하이퍼파라미터 $\\beta_2$는 0.999로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam이 적응적 학습률 알고리즘이기 때문에 학습률 하이퍼파라미터 \n",
    "$\\eta$를 튜닝할 필요가 적다.(기본값 0.001을 일반적으로 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adamax\n",
    "\n",
    "* Adam은 시간에 따라 감쇠된 그래디언트의 $l_2$노름으로 파라미터 업데이트의 스케일을 낮춘다.\n",
    "  ($l_2$노름은 제곱 합의 제곱근)\n",
    "* Adamax는 $l_2$노름을 $l_∞$노름으로 바꾼다\n",
    "\n",
    "**Equation 11-8: AdaMax algorithm**\n",
    "1. $\\mathbf{m} \\gets \\beta_1 \\mathbf{m} - (1 - \\beta_1) \\nabla_\\theta J(\\boldsymbol{\\theta})$\n",
    "2. $\\mathbf{s} \\gets \\max(\\beta_2 \\mathbf{s},\\nabla_\\theta J(\\boldsymbol{\\theta}))$\n",
    "3. $\\hat{\\mathbf{m}} \\gets \\left(\\dfrac{\\mathbf{m}}{1 - {\\beta_1}^T}\\right)$\n",
    "4. $\\hat{\\mathbf{s}} \\gets \\left(\\dfrac{\\mathbf{s}}{1 - {\\beta_2}^T}\\right)$\n",
    "5. $\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} + \\eta \\, \\hat{\\mathbf{m}} \\oslash {\\sqrt{\\hat{\\mathbf{s}} + \\epsilon}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adamax가 Adam보다 더 안정적\n",
    "* 하지만 실제로 데이터셋에 따라 다르고 일반적으로 Adam의 성능이 더 낫다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nadam\n",
    "\n",
    "* Adam 옵티마이저에 네스테로프 기법을 더한 것\n",
    "* Adam보다 조금 더 빠르게 수렴\n",
    "* 일반적으로 Nadam이 Adam보다 성능이 좋았지만 이따금 RMSProp이 나을 때도 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 최적화 기법은 1차 편미분(야코비얀)에만 의존. 2차 편미분(헤시안)을 기반으로 한 뛰어난 알고리즘들이 있지만 하나의 출력마다 n개의 1차 편미분이 아니라 \n",
    "$n^2$ 개의 2차 편미분을 계산을해야 하기 때문에 메모리 용량을 넘어서거나 계산이 너무 느려서 심층 신경망에 적용하기 어렵다.\n",
    "\n",
    "옵티마이저 비교(*=나쁨, *=보통, **=좋음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "클래스\t수렴속도\t수렴품질\n",
    "SGD\t      *\t        ***\n",
    "Momentum\t**\t      ***\n",
    "Nesterov\t**\t      ***\n",
    "AdaGrad\t  ***\t      * (너무 일찍 멈춤)\n",
    "RMSProp\t  ***\t      ** 또는 ***\n",
    "Adam\t    ***\t      ** 또는 ***\n",
    "Nadam\t    ***\t      ** 또는 ***\n",
    "AdaMax\t  ***\t      ** 또는 ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습률 스케줄링\n",
    "\n",
    "* 학습률을 너무 크게 잡으면 발산할 수 있으며, 너무 작게 잡으면 최적점에 수렴하는데 오랜 시간이 걸린다.\n",
    "\n",
    "* 큰 학습률로 시작하고 학습 속도가 느려질 때 학습률을 낮추면 최적의 고정 학습률 보다 좋은 솔루션을 더 빨리 발견할 수 있다.\n",
    "\n",
    "* 이를 학습 스케줄이라고 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 거듭제곱 스케줄링\n",
    "\n",
    "학습률을 반복 횟수 t에 대한 함수 $\\eta(t)=\\eta/(1+t/s)^c $\n",
    "로 지정. \n",
    "\n",
    "($\\eta_0$: 초기 학습률, $c$: 거듭제곱 수, $s$: 스텝 횟수)\n",
    "\n",
    "처음에는 빠르게 감소하다가 점점 더 느리게 감소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지수 기반 스케줄링\n",
    "\n",
    "$\\eta(t)=\\eta_0 0.16{t/s}$ 로 설정. 학습률이 $s$ 스텝마다 10배씩 감소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 구간별 고정 스케줄링\n",
    "\n",
    "일정 횟수의 에포크 동안 일정한 학습률을 사용하고 그 다음 \n",
    "\n",
    "또 다른 횟수의 에포크 동안 작은 학습률을 사용하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 성능 기반 스케줄링\n",
    "\n",
    "매 $N$ 스텝마다 검증 오차를 측정하고 오차가 줄어들지 않으면 $\\lambda$ 배 만큼 학습률을 감소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1사이클 스케줄링\n",
    "\n",
    "1 사이클은 훈련 절반 동안 초기 학습률 $\\eta_0$을 선형적으로 $\\eta_1$까지 증가. 나머지 절반동안 선형적으로 학습률을 $\\eta_0$까지 감소. 마지막 몇 번의 에포크는 학습률을 소수점 몇 째 자리까지 줄인다. 최대 학습률 $\\eta_1$은 최적의 학습률을 찾을 때와 같은 방식을 사용해 선택하고 초기 학습률 \n",
    "$\\eta_0$은 대략 10배 정도 낮은 값을 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**거듭제곱 스케줄링**이 가장 구현하기 쉬움. decay 매개변수만 지정하면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4) '"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4) '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**지수 기반 스케줄링과 구간별 스케줄링**도 꽤 간단함.\n",
    "\n",
    "먼저 현재 에포크를 받아 학습률을 반환하는 함수를 정의해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\eta_0$와 $s$를 하드코딩하고 싶지 않다면 이 변수를 설정한 클로저를 반환하는 함수를 만들수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그다음이 이 스케줄링 함수를 전달해 LearningRateScheduler 콜백을 만듦.\n",
    "\n",
    "그리고 이 콜백을 fit() 메서드에 전달."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs,\\n                    validation_data=(X_valid_scaled, y_valid),\\n                    callbacks=[lr_scheduler])'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 에포크마다 스텝이 많다면 스텝마다 학습률을 업데이트하는 것이 좋음.\n",
    "\n",
    "스케줄 함수는 2번째 매개변수로 현재 학습률을 받을 수 있음.\n",
    "\n",
    "예를 들어 다음과 같은 스케줄 함수는 이전 학습률에 $0.1^{1/20}$을 곱해 동일한 지수감쇠효과를 냄.\n",
    "\n",
    "여기서는 에포크가 1이 아닌 0에서부터 감쇠가 시작됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1**(1 / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 구현은 (이전 구현과 달리) 옵티마이저의 초기 학습률에만 의존하므로 이를 적절히 설정해야함.\n",
    "\n",
    "하지만 스케줄 함수가 epoch 매개변수를 사용하면 문제가 좀 복잡해짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 규제를 사용해 과대적합 피하기\n",
    "\n",
    "10장에서 이미 최상위 규제 방법 중 하나인 조기 종료를 구현함.\n",
    "\n",
    "또한 배치 정규화는 불안정한 그레디언트 문제를 해결하기 위해 고안되었지만 \n",
    "\n",
    "꽤 괜찮은 규제방법으로도 사용가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\ell_1$과 $\\ell_2$규제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 17s 9ms/step - loss: 1.5922 - accuracy: 0.8122 - val_loss: 0.7183 - val_accuracy: 0.8328\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.7189 - accuracy: 0.8276 - val_loss: 0.6802 - val_accuracy: 0.8382\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(100, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 네트워크의 모든 은닉층에 동일한 활성화함수, 동일한 초기화 전략을 사용하거나\n",
    "\n",
    "모든 층에 동일한 규제를 적용하기 때문에 동일한 매개변수 값을 반복하는 경우가 많음.\n",
    "\n",
    "=> 리팩터링 또는 파이썬의 functools.partial()를 사용해 기본 매개변수 값을 사용해 함수 호출 감쌈."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 1.6637 - accuracy: 0.8123 - val_loss: 0.7172 - val_accuracy: 0.8336\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.7168 - accuracy: 0.8285 - val_loss: 0.6838 - val_accuracy: 0.8370\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 드롭아웃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매 훈련스텝에서 각 뉴런(입력뉴런은 포함하고 출력뉴런은 제외함)은 \n",
    "\n",
    "임시적으로 드롭아웃될 확률 $p$를 가짐. 즉, 이번 훈련스텝에는 완전 무시되지만 \n",
    "\n",
    "다음 스텝에는 활성화될 수 있음. 하이퍼파라미터 $p$를 드롭아웃 비율이라고 하고\n",
    "\n",
    "보통 10%~50% 사이를 지정. 순환신경망에서는 20~30%에 가깝고 합성곱 신경망에서는 40~50%에 가까움.\n",
    "\n",
    "훈련이 끝난 후에는 뉴런에 더는 드롭아웃을 적용하지 않음. 이게 전부임 (세부사항은 뒤에 설명)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "드롭아웃의 능력을 이해하는 또 다른방법은 각 훈련스텝에서 고유한 네트워크가 생성된다고 생각하는 것.\n",
    "\n",
    "개개의 뉴런이 있을수도 없을수도 있기 때문에 $2^N$개의 네트워크가 가능함(N은 드롭아웃이 가능한 뉴런수).\n",
    "\n",
    "이는 아주 큰 값이어서 같은 네트워크가 2번 선택될 가능성이 사실상 거의 없음. \n",
    "\n",
    "|\n",
    "\n",
    "1만번의 훈련스텝을 진행하면 1만개의 다른 신경망을 (각각 하나의 훈련샘플을 사용해) 훈련하게 됨.\n",
    "\n",
    "이 신경망은 대부분의 가중치를 공유하고 있기 때문에 아주 독립적이지 않음. 하지만 그럼에도 모두 다름.\n",
    "\n",
    "결과적으로 만들어진 신경망은 이 모든 신경망을 평균한 앙상블로 볼 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항\n",
    "\n",
    "p=50%로 설정되었을때, 특정 뉴런은 평균적으로 두배 많은 입력과 연결되므로, 이런 부분을 보정하기위해, 훈련이 끝난 이후, 연결 가중치에 0.5를 곱함\n",
    "\n",
    "> 즉, 훈련이 끝난 뒤, 연결 가중치에 보존 확률 (1-p)를 곱한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 가지 사소하나 중요한 기술적 세부사항이 있는데 이를 일반적으로 말하자면\n",
    "\n",
    "훈련이 끝난 뒤 각 입력의 연결 가중치에 보존확률 (1-$p$)을 곱해야 함. 또는 훈련하는 동안\n",
    "\n",
    "각 뉴런의 출력을 보존확률로 나눌수도 있음(이 방식이 완전히 같은 것은 아니지만 잘 작동).\n",
    "\n",
    "|\n",
    "\n",
    "keras.layers.Dropout 층을 사용해 드롭아웃을 구현함. 이 층은 훈련하는 동안 일부 입력을 랜덤하게 버림.\n",
    "\n",
    "그다음 남은 입력을 보존확률로 나눔. 훈련이 끝난 후에는 어떤 작업도 하지 않음.\n",
    "\n",
    "입력을 다음층으로 그냥 전달. \n",
    "\n",
    "아래 코드는 드롭아웃 비율 0.2를 사용한 규제를 모든 Dense 층 이전에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.5716 - accuracy: 0.8030 - val_loss: 0.3666 - val_accuracy: 0.8676\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 15s 8ms/step - loss: 0.4215 - accuracy: 0.8455 - val_loss: 0.3499 - val_accuracy: 0.8690\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 과대적합되었다면 드롭아웃 비율을 늘릴 수 있음. 반대의 상황이면 비율을 낮춰야 함.\n",
    "\n",
    "층이 클 때는 드롭아웃 비율을 늘리고 작은 층에는 비율을 줄임.\n",
    "\n",
    "최신의 신경망구조는 마지막 은닉층 뒤에만 드롭아웃을 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 알파 드롭아웃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.6602 - accuracy: 0.7582 - val_loss: 0.6103 - val_accuracy: 0.8410\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5559 - accuracy: 0.7946 - val_loss: 0.5654 - val_accuracy: 0.8386\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5238 - accuracy: 0.8045 - val_loss: 0.5221 - val_accuracy: 0.8482\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5044 - accuracy: 0.8133 - val_loss: 0.4759 - val_accuracy: 0.8620\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4946 - accuracy: 0.8153 - val_loss: 0.4371 - val_accuracy: 0.8598\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4823 - accuracy: 0.8228 - val_loss: 0.4679 - val_accuracy: 0.8606\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4718 - accuracy: 0.8258 - val_loss: 0.4606 - val_accuracy: 0.8620\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.4672 - accuracy: 0.8279 - val_loss: 0.4471 - val_accuracy: 0.8610\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4602 - accuracy: 0.8291 - val_loss: 0.4378 - val_accuracy: 0.8656\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4503 - accuracy: 0.8321 - val_loss: 0.4350 - val_accuracy: 0.8636\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4514 - accuracy: 0.8317 - val_loss: 0.4225 - val_accuracy: 0.8680\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4459 - accuracy: 0.8340 - val_loss: 0.5069 - val_accuracy: 0.8572\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4419 - accuracy: 0.8353 - val_loss: 0.4361 - val_accuracy: 0.8748\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4393 - accuracy: 0.8365 - val_loss: 0.4470 - val_accuracy: 0.8660\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4336 - accuracy: 0.8400 - val_loss: 0.4226 - val_accuracy: 0.8706\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4315 - accuracy: 0.8392 - val_loss: 0.4391 - val_accuracy: 0.8702\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4310 - accuracy: 0.8394 - val_loss: 0.5272 - val_accuracy: 0.8556\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4249 - accuracy: 0.8415 - val_loss: 0.4755 - val_accuracy: 0.8736\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4234 - accuracy: 0.8427 - val_loss: 0.4477 - val_accuracy: 0.8692\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4187 - accuracy: 0.8446 - val_loss: 0.4074 - val_accuracy: 0.8728\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4434 - accuracy: 0.8601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44340434670448303, 0.8600999712944031]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3277 - accuracy: 0.8821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32773226499557495, 0.8820727467536926]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4215 - accuracy: 0.8428\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 몬테 카를로 드롭아웃 (MC 드롭아웃)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측과정에서 일반 dropout기법과 다르게 dropout층을 활성화 시키고, 테스트 셋의 100번 예측 결과를 만듦\n",
    "\n",
    "모델을 호출할 때마다 샘플이 행이고 클래스마다 하나의 열을 가진 행이 반환.\n",
    "\n",
    "테스트 세트에 1만개 샘플과 10개의 클래스가 있으므로 이 행렬의 크기는 [10000, 10].\n",
    "\n",
    "이런 행렬 100개를 쌓았기 때문에 y_probas는 [100, 10000, 100] 크기의 행렬.\n",
    "\n",
    "첫번째 차원(axis=0)을 기준으로 평균하면 한 번의 예측을 수행했을 때와 같은 \n",
    "\n",
    "[10000 ,100] 크기의 배열 y_proba를 얻게 됨. 이게 끝!\n",
    "\n",
    "---\n",
    "\n",
    "드롭아웃으로 만든 예측을 평균하면 일반적으로 드롭아웃이 없이 \n",
    "\n",
    "예측한 하나의 결과보다 더 안정적인것 확인 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 드롭아웃을 끄고 패션 MNIST 테스트 세트에 있는 첫번째 샘플의 모델 예측 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델은 이 이미지가 클래스 9가 맞다고 확신, 정말로 의심의 여지가 없나?\n",
    "\n",
    "다시 드롭아웃 활성화해 만든 예측과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.01, 0.  , 0.71]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.15, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.02, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.2 , 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.62, 0.  , 0.26]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.65, 0.  , 0.16, 0.  , 0.19]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.35, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.24, 0.  , 0.26, 0.  , 0.5 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.03, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.48, 0.  , 0.33]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.01, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.36, 0.  , 0.56]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.27, 0.  , 0.04, 0.  , 0.69]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.36, 0.  , 0.31, 0.  , 0.33]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.26, 0.  , 0.24]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.43, 0.  , 0.18, 0.  , 0.39]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.01, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.05, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.66, 0.  , 0.17, 0.  , 0.17]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.18, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.56, 0.  , 0.39]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.07, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.35, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.27, 0.  , 0.71]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.19, 0.  , 0.2 , 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.37, 0.  , 0.54]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.09, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.13, 0.  , 0.7 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.05, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.12, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.39, 0.  , 0.58]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.03, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.03, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.05, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.65, 0.  , 0.14, 0.  , 0.22]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.05, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.38, 0.  , 0.19, 0.  , 0.43]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.51, 0.  , 0.04, 0.  , 0.45]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.27, 0.  , 0.57, 0.  , 0.16]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.45, 0.  , 0.23]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.1 , 0.  , 0.84]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.44, 0.  , 0.06]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.21, 0.  , 0.71]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.72, 0.  , 0.21]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.02, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.03, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.16, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.33, 0.  , 0.57]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.65, 0.  , 0.23, 0.  , 0.11]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.29, 0.  , 0.06, 0.  , 0.65]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.07, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.6 , 0.  , 0.24]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.34, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.09, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.17, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.35, 0.  , 0.33, 0.  , 0.32]],\n",
       "\n",
       "       [[0.01, 0.  , 0.01, 0.  , 0.03, 0.08, 0.  , 0.03, 0.01, 0.83]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.11, 0.  , 0.76]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.24, 0.  , 0.42, 0.  , 0.33]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.  , 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.97, 0.  , 0.  , 0.  , 0.02]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.43, 0.  , 0.56]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.5 , 0.  , 0.49]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.43, 0.  , 0.3 , 0.  , 0.27]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.04, 0.  , 0.68]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.63, 0.  , 0.32]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.24, 0.  , 0.73]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.32, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.22, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.02, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.08, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.02, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.83, 0.  , 0.14]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.58, 0.  , 0.31, 0.  , 0.11]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.03, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.14, 0.  , 0.71]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.5 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.19, 0.  , 0.21, 0.  , 0.6 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.19, 0.  , 0.68]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.25, 0.  , 0.67]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.2 , 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.12, 0.  , 0.81]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.23, 0.  , 0.45]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.06, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.2 , 0.  , 0.74]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probas[:, :1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "드롭아웃 활성화 시 모델이 더 이상 확신하지 않음. 여전히 클래스 9를 선호하나\n",
    "\n",
    "이따금 클래스 5나 7로 생각. 이들은 모두 신발이니 공통점이 존재함. 첫번째 차원으로 평균을 내면\n",
    "\n",
    "MC드롭아웃 예측은 다음과 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.19, 0.  , 0.67]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여전히 클래스 9에 속한다고 생각하지만 67%만 확신.\n",
    "\n",
    "또한 모델이 다른 클래스에 대해 정확히 알 수 있음. 이 확률추정의 표준분포를 확인할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.19, 0.  , 0.19, 0.  , 0.28]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 확률추정에는 많은 분산이 있음. 만약 위험에 민감한 시스템(금융, 의료 등등)을\n",
    "\n",
    "만든다면 이런 불확실한 예측을 매우 주의 깊게 다뤄야 함.\n",
    "\n",
    "모델의 정확도도 86.9%로 상승."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.869"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 훈련하는 동안 다르게 작동하는 층(BatchNormalization 층과 같은)을 가지고 있다면\n",
    "\n",
    "앞에서와 같이 훈련모드를 강제로 설정해선 안 됨. 대신 Dropout층을 다음과 같은 MCDropout로 바꿔야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout층을 상속하고, call()메서드를 이용해 오버라이드하여 training매개변수를 강제로 True로 설정함.\n",
    "\n",
    "간단히 말해서 MC드롭아웃은 드롭아웃 모델의 성능을 높여주고, 더 정확한 불확실성 추정을 제공하는 기술."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_21 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " mc_alpha_dropout_3 (MCAlpha  (None, 784)              0         \n",
      " Dropout)                                                        \n",
      "                                                                 \n",
      " dense_382 (Dense)           (None, 300)               235500    \n",
      "                                                                 \n",
      " mc_alpha_dropout_4 (MCAlpha  (None, 300)              0         \n",
      " Dropout)                                                        \n",
      "                                                                 \n",
      " dense_383 (Dense)           (None, 100)               30100     \n",
      "                                                                 \n",
      " mc_alpha_dropout_5 (MCAlpha  (None, 100)              0         \n",
      " Dropout)                                                        \n",
      "                                                                 \n",
      " dense_384 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 0.  , 0.19, 0.  , 0.63]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 맥스-노름 규제\n",
    "\n",
    "케라스에서 맥스-노름 규제를 구현하려면 다음처럼 적절한 최댓값으로 지정한 max_norm()이\n",
    "\n",
    "반환한 객체로 은닉층의 kernel_constraint 매개변수를 지정해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                           kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매 훈련 반복이 끝난 후 모델의 fit() 메서드가 층의 가중치와 함께 max_norm()이 반환한\n",
    "\n",
    "객체를 호출하고 스케일이 조정된 가중치를 반환받음. 이 값을 사용해 가중치를 바꿈.\n",
    "\n",
    "12장에서 보겠지만 필요하면 사용자 정의 규제 함수를 정의함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.4752 - accuracy: 0.8331 - val_loss: 0.3684 - val_accuracy: 0.8654\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.3556 - accuracy: 0.8703 - val_loss: 0.3735 - val_accuracy: 0.8664\n"
     ]
    }
   ],
   "source": [
    "MaxNormDense = partial(keras.layers.Dense,\n",
    "                       activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    MaxNormDense(300),\n",
    "    MaxNormDense(100),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약 및 실용적인 가이드라인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기본적인 DNN 설정**\n",
    "\n",
    "* 커널 초기화 > He 초기화\n",
    "* 활성화 함수 > ELU\n",
    "* 정규화 > 얕은 신경일 경우 없음. 깊은 신경망이라면 배치 정규화\n",
    "* 규제 > 조기종료(필요하다면 $\\ell_2$ 규제 추가)\n",
    "* 옵티마이저 > 모멘텀 최적화(또는 RMSProp이나 Nadam)\n",
    "* 학습률 스케줄 > 1사이클"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**자기정규화를 위한 DNN 설정**\n",
    "\n",
    "* 커널 초기화 > 르쿤 초기화\n",
    "* 활성화 함수 > SELU\n",
    "* 정규화 > 없음 (자기 정규화)\n",
    "* 규제 > 필요하다면 알파드롭\n",
    "* 옵티마이저 > 모멘텀 최적화(또는 RMSProp이나 Nadam)\n",
    "* 학습률 스케줄 > 1사이클"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**가이드라인의 몇가지 예외적인 경우 존재**\n",
    "\n",
    "* 희소모델이 필요하다면 $\\ell_1$규제를 사용가능. 매우 희소한 모델이 필요하면 텐서플로 모델 최적화\n",
    "  툴킷(TF-MOT)을 사용할 수 있음. 이 도구는 자기 정규화를 깨뜨리므로 이 경우 기본 DNN 설정 사용해야함.\n",
    "\n",
    "* 빠른 응답을 하는 모델(번개처럼 예측하는 모델)이 필요하면 층 개수를 줄이고 배치정규화 층을 이전 층에 \n",
    "  합치는 것. LeakyReLU나 ReLU와 같이 빠른 활성화 함수 사용. 희소 모델을 만드는 것도 도움됨.\n",
    "  마지막으로 부동소수점 정밀도를 32비트에서 16비트 혹은 8비트로 낮출수도 있음.\n",
    "  여기서도 TF-MOT 확인 필요.\n",
    "\n",
    "* 위험에 민감하고 예측 속도가 매우 중요하지 않은 애플리케이션이라면 성능을 올리고 불확실성 추정과\n",
    "  신뢰가능한 확률추정을 얻기 위해 MC 드롭아웃을 사용할 수 있음."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916268a5c6dcd25200736720d94968f5c0533c421c8ed3589928c0deacd85b5f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('tf_pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
