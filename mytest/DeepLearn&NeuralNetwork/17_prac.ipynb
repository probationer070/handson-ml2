{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 그림을 저장할 위치\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"autoencoders\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"그림 저장\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 텐서 플로의 정보 출력 억제하기\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # GPU 장치 지정\n",
    "\n",
    "# tf.debugging.set_log_device_placement(True)   # 이거 쓰지 마셈 ㅈㄴ 출력 더러움\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"감지된 GPU가 없습니다. GPU가 없으면 LSTM과 CNN이 매우 느릴 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오토인코더와 GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 오토인코더 \n",
    "\n",
    "- 단순히 입력을 출력으로 복사하는 방법을 배움\n",
    "\n",
    "- 네트워크에 제약을 가해, 이 작업을 어렵게 만듦 (잠재 표현의 크기를 제한하거나, 입력에 잡음을 추가하고 원본 입력을 복원하도록 네트워크를 훈련)\n",
    "\n",
    "- 오토인코더가 바로 복사하지 못하고, 데이터를 효율적으로 표현하는 방법을 배움\n",
    "\n",
    "- 즉, 코딩은 일정 제약 조건하에, 항등 함수를 학습하려는 오토인코더의 노력으로 생긴 부산물\n",
    "\n",
    " \n",
    "\n",
    "##### GAN\n",
    "\n",
    "- 생성자와 판별자로 구성\n",
    "\n",
    "- 생성자는 훈련 데이터와 비슷하게 보이는 데이터를 생성\n",
    "\n",
    "- 판별자는 가짜 데이터와 진짜 데이터를 구별함\n",
    "\n",
    "- 신경망이 훈련하는 동안 생성자와 판별자가 서로 경쟁함 -> 적대적 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 효율적인 데이터 표현하기\n",
    "\n",
    "보통은 긴 시퀀스를 기억하기 어렵기 때문에 패턴을 찾는 것이 유용.\n",
    "\n",
    "기억, 지각, 패턴 매칭 사이의 관계에 관한 연구 -> \n",
    "\n",
    "숙련된 체스 플레이어가 체스판을 5초만 보고도 전체 말의 위치를 외울 수 있다.\n",
    "\n",
    "어떻게? 무작위로 놓였을 있을 때가 아니라 현실적인 위치에 있을 때.\n",
    "\n",
    "체스 전문가라고 기억력이 뛰어난 것이 아님.\n",
    "\n",
    "=> 즉 게임에 대한 경험 덕에 체스 패턴을 쉽게 보는 것.(패턴을 찾으면 정보를 효울적으로 저장가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\images\\\\autoencoders\\\\auto_1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\learn\\Math\\handson-ml2_free\\mytest\\DeepLearn&NeuralNetwork\\17_prac.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/learn/Math/handson-ml2_free/mytest/DeepLearn%26NeuralNetwork/17_prac.ipynb#ch0000053?line=2'>3</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto_1.png\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/learn/Math/handson-ml2_free/mytest/DeepLearn%26NeuralNetwork/17_prac.ipynb#ch0000053?line=3'>4</a>\u001b[0m images_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(PROJECT_ROOT_DIR, \u001b[39m\"\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\"\u001b[39m, CHAPTER_ID)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/learn/Math/handson-ml2_free/mytest/DeepLearn%26NeuralNetwork/17_prac.ipynb#ch0000053?line=4'>5</a>\u001b[0m show_img \u001b[39m=\u001b[39m mpimg\u001b[39m.\u001b[39;49mimread(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(images_path, filename))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/learn/Math/handson-ml2_free/mytest/DeepLearn%26NeuralNetwork/17_prac.ipynb#ch0000053?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m20\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/learn/Math/handson-ml2_free/mytest/DeepLearn%26NeuralNetwork/17_prac.ipynb#ch0000053?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m\"\u001b[39m\u001b[39moff\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\tf_pt\\lib\\site-packages\\matplotlib\\image.py:1560\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Anaconda3/envs/tf_pt/lib/site-packages/matplotlib/image.py?line=1557'>1558</a>\u001b[0m                 response \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(response\u001b[39m.\u001b[39mread())\n\u001b[0;32m   <a href='file:///c%3A/Anaconda3/envs/tf_pt/lib/site-packages/matplotlib/image.py?line=1558'>1559</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m imread(response, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39mext)\n\u001b[1;32m-> <a href='file:///c%3A/Anaconda3/envs/tf_pt/lib/site-packages/matplotlib/image.py?line=1559'>1560</a>\u001b[0m \u001b[39mwith\u001b[39;00m img_open(fname) \u001b[39mas\u001b[39;00m image:\n\u001b[0;32m   <a href='file:///c%3A/Anaconda3/envs/tf_pt/lib/site-packages/matplotlib/image.py?line=1560'>1561</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[0;32m   <a href='file:///c%3A/Anaconda3/envs/tf_pt/lib/site-packages/matplotlib/image.py?line=1561'>1562</a>\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mPngImagePlugin\u001b[39m.\u001b[39mPngImageFile) \u001b[39melse\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Anaconda3/envs/tf_pt/lib/site-packages/matplotlib/image.py?line=1562'>1563</a>\u001b[0m             pil_to_array(image))\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\tf_pt\\lib\\site-packages\\PIL\\ImageFile.py:104\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[1;34m(self, fp, filename)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda3/envs/tf_pt/lib/site-packages/PIL/ImageFile.py?line=99'>100</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodermaxblock \u001b[39m=\u001b[39m MAXBLOCK\n\u001b[0;32m    <a href='file:///c%3A/Anaconda3/envs/tf_pt/lib/site-packages/PIL/ImageFile.py?line=101'>102</a>\u001b[0m \u001b[39mif\u001b[39;00m isPath(fp):\n\u001b[0;32m    <a href='file:///c%3A/Anaconda3/envs/tf_pt/lib/site-packages/PIL/ImageFile.py?line=102'>103</a>\u001b[0m     \u001b[39m# filename\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Anaconda3/envs/tf_pt/lib/site-packages/PIL/ImageFile.py?line=103'>104</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(fp, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Anaconda3/envs/tf_pt/lib/site-packages/PIL/ImageFile.py?line=104'>105</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m fp\n\u001b[0;32m    <a href='file:///c%3A/Anaconda3/envs/tf_pt/lib/site-packages/PIL/ImageFile.py?line=105'>106</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\images\\\\autoencoders\\\\auto_1.png'"
     ]
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "filename = \"auto_1.png\"\n",
    "images_path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "show_img = mpimg.imread(os.path.join(images_path, filename))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(show_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오토인코더는 입력을 받아 효율적인 내부 표현으로 바꾸고 입력과 가장 가까운 어떤 것을 출력함\n",
    "\n",
    "오토인코더는 항상 두 부분으로 구성\n",
    "\n",
    "- 입력을 내부 표현으로 바꾸는 인코더 (인지 네트워크)\n",
    "\n",
    "- 내부 표현을 출력으로 바꾸는 디코더 (생성 네트워크)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"auto_2.png\"\n",
    "images_path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "show_img = mpimg.imread(os.path.join(images_path, filename))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(show_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력층의 뉴런 수가 입력 개수와 동일하다는 것을 제외하면, 오토인코더는 다층 퍼셉트론과 구조가 동일\n",
    "\n",
    "- 뉴런 두 개로 구성된 하나의 은닉층 (인코더)\n",
    "\n",
    "- 뉴런 세 개로 구성된 출력층 (디코더)\n",
    "\n",
    "- 오토인코더가 입력을 재구성하기 때문에 출력을 재구성 이라고 부르고,\n",
    "\n",
    "- 비용 함수는 재구성이 입력과 다를 때 모델에 벌점을 부과하는 재구성 손실을 포함함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내부의 표현이 입력 데이터보다 저차원(3차원 -> 2차원)이기 때문에, \n",
    "\n",
    "이런 오토인코더를 과소완전(undercomplete) 이라고 함\n",
    "\n",
    "- 과소완전 오토인코더는 입력을 코딩으로 간단히 복사할 수 없으며, \n",
    "  \n",
    "  입력과 똑같은 것을 출력하기 위한 방법을 찾아야함\n",
    "\n",
    "-> 이를 통해 입력 데이터에서 가장 중요한 특성을 학습하도록 만듦\n",
    "\n",
    "- 적어도 입력 데이터에 대해서는 복원(재구성)을 잘한다는 특징이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과소완전 선형 오토인코더로 PCA 수행하기\n",
    "\n",
    "오토인코더가 선형 활성화 함수만 사용하고 비용 함수가 MSE라면,\n",
    "\n",
    "이는 결국 PCA(8장 참조)를 수행하는 것을 볼 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28x28 흑백 이미지를 그리기 위한 유틸리티 함수:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "\n",
    "def generate_3d_data(m, w1=0.1, w2=0.3, noise=0.1):\n",
    "    angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "    data = np.empty((m, 3))\n",
    "    data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "    data[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "    data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * np.random.randn(m)\n",
    "    return data\n",
    "\n",
    "X_train = generate_3d_data(60)\n",
    "X_train = X_train - X_train.mean(axis=0, keepdims=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D 데이터셋에 PCA를 적용해 2D에 투영하는 간단한 선형 오토인코더를 만듦."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])])\n",
    "decoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])])\n",
    "autoencoder = keras.models.Sequential([encoder, decoder])\n",
    "\n",
    "autoencoder.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 오토인코더를 인코더와 디코더 2개 컴포넌트로 구성. 둘 다 일반적인 Sequential 모델.\n",
    "\n",
    "  오토인코더는 인코더 다음에 디코더가 뒤따르는 Sequential 모델.\n",
    "\n",
    "* 오토인코더의 출력 개수 = 입력 개수와 동일 (즉, 3개).\n",
    "\n",
    "* 단순한 PCA를 수행하기 위해선 활성화 함수를 사용하지 않으며(즉, 모든 뉴런이 선형). 비용함수는 MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D 데이터셋에 훈련 후 모델을 사용해 동일한 데이터셋을 인코딩 (= 2D로 투영) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(X_train, X_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings = encoder.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "plt.plot(codings[:,0], codings[:, 1], \"b.\")\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "save_fig(\"linear_autoencoder_pca_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"auto_3.png\"\n",
    "images_path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "show_img = mpimg.imread(os.path.join(images_path, filename))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(show_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오토인코더는 PCA처럼 데이터에 있는 분산이 가능한 많이 보존되도록 데이터를 투영할 최상의 2D 평면을 찾음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 적층 오토인코더\n",
    "\n",
    "은닉층을 여러 개 가지는 오토인코더\n",
    "\n",
    "- 층을 더 추가하여 더 복잡한 코딩을 학습할 수 있음\n",
    "\n",
    "- 오토인코더가 너무 강력해지지 않도록 주의\n",
    "  - 인코더가 너무 강력해서 각각의 입력 데이터를 임의의 한 숫자로 매핑하도록 학습했다고 가정\n",
    "  - 훈련 데이터를 완벽히 재구성하겠지만, 유용한 데이터 표현을 학습하지 못할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"auto_4.png\"\n",
    "images_path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "show_img = mpimg.imread(os.path.join(images_path, filename))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(show_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 적층 오토인코더는 전형적으로 가운데 은닉층(코딩 층)을 기준으로 대칭\n",
    "\n",
    "- 위 예시는 입력 784개, 뉴런 300개로 된 은닉층, 뉴런 150개로 된 가운데 은닉층, 뉴런 300개로 된 은닉층, 뉴런 784개로 된 출력층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 데이터셋을 사용합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 케라스를 사용해 적층 오토인코더 구현하기\n",
    "\n",
    "3개의 은닉층과 1개의 출력층(즉, 두 개를 적층)을 가진 적층 오토인코더를 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounded_accuracy(y_true, y_pred):\n",
    "    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "stacked_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(30, activation=\"selu\"),\n",
    "])\n",
    "stacked_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
    "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\n",
    "stacked_ae.compile(loss=\"binary_crossentropy\",\n",
    "                   optimizer=keras.optimizers.SGD(learning_rate=1.5), metrics=[rounded_accuracy])\n",
    "history = stacked_ae.fit(X_train, X_train, epochs=20,\n",
    "                         validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오토인코더 모델을 인코더와 디코더 두 모델로 나눔\n",
    "\n",
    "- 인코더는 28x28 픽셀의 흑백 이미지를 받고, 이미지를 784 크기의 벡터로 표현하기 위해 펼침\n",
    "  \n",
    "  SELU 활성화 함수를 사용하는 Dense 층 두 개에 통과시킴 \n",
    "  \n",
    "  (르쿤 정규분포 초기화를 추가할 수 있으나 차이가 크지 않음 네트워크가 깊지 않아서)\n",
    "\n",
    "  각 입력 이미지에 대해 인코더는 크기가 30인 벡터를 출력함\n",
    "\n",
    "- 디코더는 크기가 30인 코딩을 받아, 크기가 커지는 Dense 층 두 개에 통과시킴\n",
    "\n",
    "  최종 벡터를 28x28 배열로 변경하여 디코더의 출력이 인코더의 입력과 동일한 크기가 되도록 함\n",
    "\n",
    "- 적층 인코더를 컴파일할 때 평균 제곱 오차 대신 이진 크로스 엔트로피 손실을 사용\n",
    "\n",
    "  - 재구성 작업을 다중 레이블 이진 분류 문제로 다루는 것\n",
    "  \n",
    "  - 각 필셀의 강도는 픽셀이 검정일 확률을 나타냄\n",
    "\n",
    "- X_train을 입력과 타깃으로 사용해 모델을 훈련함 (X_valid를 검증 입력과 검증 타킷으로 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 재구성 시각화\n",
    "\n",
    "오토인코더가 잘 훈련되었는지 확인하는 방법은 입력과 출력을 비교하는 것\n",
    "\n",
    "- 입력과 출력의 차이가 크지 않아야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def show_reconstructions(model, images=X_valid, n_images=5):\n",
    "    reconstructions = model.predict(images[:n_images])\n",
    "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plot_image(images[image_index])\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plot_image(reconstructions[image_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(stacked_ae)\n",
    "save_fig(\"reconstruction_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 재구성된 이미지를 식별할 수는 있지만, 정보를 많이 읽은 모습\n",
    "\n",
    "- 모델을 더 훈련하고 인코더/디코더 층을 늘리거나 코딩을 늘리면 나아질 수 있으나, 네트워크가 너무 강력하면 데이터에서 유익한 패턴을 학습하지 못하고 완벽한 재구성 이미지를 만드려고할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 패션 MNIST 데이터셋 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련한 적층 오토인코더 모델을 사용해, 데이터 셋의 차원을 축소할 수 있음\n",
    "\n",
    "- 시각화 입장에서 보면, 다른 차원 축소 알고리즘만큼의 성능은 아니지만\n",
    "\n",
    "- 샘플과 특성이 많은 대용량 데이터셋을 다룰 수 있음\n",
    "\n",
    "- 오토인코더를 사용해 적절히 차원을 축소하고, 다른 차원 축소 알고리즘을 사용해 시각화하는 것도 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패션 MNIST 데이터셋 시각화\n",
    "\n",
    "- 적층 오토인코더의 인코더 모델을 사용해 차원을 30으로 줄이고,\n",
    "\n",
    "- t-SNE 알고리즘을 구현한 사이킷런 클래스를 통해 차원을 2까지 줄임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_valid_compressed = stacked_encoder.predict(X_valid)\n",
    "tsne = TSNE()\n",
    "X_valid_2D = tsne.fit_transform(X_valid_compressed)\n",
    "X_valid_2D = (X_valid_2D - X_valid_2D.min()) / (X_valid_2D.max() - X_valid_2D.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **왜 대용량 고차원 데이터셋을 차원 축소하는 데 적합한가?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8장 차원축소 장에서 잠깐 다루었던 LLE, Isomap 등은 각 샘플 별로 근방에 있는 데이터를 사용하는 neighborhood 기반 방법임\n",
    "\n",
    "-> 즉, 가까운 애들이 manifold 상에서도 가까울 것이라고 가정하는 것\n",
    "\n",
    "-> 하지만, 의미적으로 가까울 것이라고 생각하는 샘플들이 가깝지 않을 수 있음\n",
    "\n",
    "  (고차원 데이터 간의 유클리디안 거리 != 유의미한 거리)\n",
    "\n",
    "-> 데이터 공간이 고차원일 수록, 매니폴드를 찾기 어려울 수록\n",
    "\n",
    "-> neighborhood방식이 아닌 DNN 기반의 오토인코더가 유용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=\"tab10\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html 참고\n",
    "plt.figure(figsize=(10, 8))\n",
    "cmap = plt.cm.tab10\n",
    "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=cmap)\n",
    "image_positions = np.array([[1., 1.]])\n",
    "for index, position in enumerate(X_valid_2D):\n",
    "    dist = np.sum((position - image_positions) ** 2, axis=1)\n",
    "    if np.min(dist) > 0.02: # if far enough from other images\n",
    "        image_positions = np.r_[image_positions, [position]]\n",
    "        imagebox = mpl.offsetbox.AnnotationBbox(\n",
    "            mpl.offsetbox.OffsetImage(X_valid[index], cmap=\"binary\"),\n",
    "            position, bboxprops={\"edgecolor\": cmap(y_valid[index]), \"lw\": 2})\n",
    "        plt.gca().add_artist(imagebox)\n",
    "plt.axis(\"off\")\n",
    "save_fig(\"fashion_mnist_visualization_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 적층 오토인코더를 사용한 비지도 사전훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블된 훈련 데이터가 많지 않을때 적층 오토인코더를 통해 훈련하기 \n",
    "\n",
    "- 대부분 레이블되지 않은 대량의 데이터셋을, 먼저 전체 데이터셋을 사용해 적층 오토인코더를 훈련\n",
    "\n",
    "- 그 다음, 오토인코더의 하위층을 재사용해 실제 문제를 해결하기 위한 신경망을 만들고, 레이블된 데이터를 사용해 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구현도 아래와 같이 가능\n",
    "\n",
    "1. 레이블된 것, 레이블되지 않은 모든 훈련 데이터를 사용해 오토인코더를 훈련\n",
    "\n",
    "2. 그 후 인코더 층을 재사용하여 새로운 신경망을 만듦\n",
    "\n",
    "3. 레이블된 데이터를 통해 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가중치 묶기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예제처럼, 오토인코더가 대칭일 땐, 디코더의 가중치와 인코더의 가중치를 묶는 것이 일반적\n",
    "\n",
    "- 모델에 있는 가중치의 수를 절반으로 줄여 훈련 속도를 높이고, 과대적합의 위험을 줄임\n",
    "\n",
    " \n",
    "\n",
    "어떤 오토인코더가 $N$ 개 층을 갖고, $W_L$ 이 $L$ 번째 층의 가중치를 나타낼 때\n",
    "\n",
    "- (1은 첫 번째 은닉층, $N/2$ 은 코딩 층, $N$ 은 출력층)\n",
    "\n",
    "- 디코더 층의 가중치는 $W_{N-L}+1 = W_L^T (L = 1,2, ..., N/2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **케라스의 사용자 정의 층을 만들어 층 간 가중치 묶기 :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTranspose(keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        self.dense = dense\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name=\"bias\",\n",
    "                                      shape=[self.dense.input_shape[-1]],\n",
    "                                      initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)  \n",
    "        # transpose_b=True로 지정하는 것과 동일하나 matmul()이 더 효율적\n",
    "        \n",
    "        return self.activation(z + self.biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적인 Dense 층과 비슷하지만, 다른 Dense 층의 전치된 가중치를 사용함\n",
    "\n",
    "- 편향 벡터는 독자적으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **위 사용자 정의층을 사용하여 적층 오토인코더 구성 :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "dense_1 = keras.layers.Dense(100, activation=\"selu\")\n",
    "dense_2 = keras.layers.Dense(30, activation=\"selu\")\n",
    "\n",
    "tied_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    dense_1,\n",
    "    dense_2\n",
    "])\n",
    "\n",
    "tied_decoder = keras.models.Sequential([\n",
    "    DenseTranspose(dense_2, activation=\"selu\"),\n",
    "    DenseTranspose(dense_1, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "tied_ae = keras.models.Sequential([tied_encoder, tied_decoder])\n",
    "\n",
    "tied_ae.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1.5), metrics=[rounded_accuracy])\n",
    "history = tied_ae.fit(X_train, X_train, epochs=10,\n",
    "                      validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 모델은 절반의 파라미터로 이전 모델보다 약간 낮은 내구성 오차를 달성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(tied_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한 번에 오토인코더 한 개씩 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 번에 전체 오토인코더를 훈련하지 않고, 오토인코더 하나를 훈련하고 이를 쌓아올려서 한 개의 적층 오토인코더를 만들 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재는 많이 사용 x\n",
    "\n",
    "- '탐욕적 방식의 층별 훈련' 에 대한 논문에 등장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"auto_5.png\"\n",
    "images_path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "show_img = mpimg.imread(os.path.join(images_path, filename))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(show_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단계 1 에서 첫 번째 오토인코더는 입력을 재구성하도록 학습\n",
    "\n",
    "- 이 오토인코더를 사용해 전체 훈련 세트는 인코딩하여 (압축된) 새 훈련 세트를 만듦\n",
    "\n",
    "- 새로운 훈련 세트에서 두 번째 오토인코더를 훈련 (단계 2)\n",
    "\n",
    "- 마지막으로 모든 오토인코더를 사용해 전체 네트워크를 만듦 (단계 3, 각 오토인코더의 은닉층을 먼저 쌓고, 그 다음 출력층을 (반대로) 쌓음) -> 최종 적층 오토인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(n_neurons, X_train, X_valid, loss, optimizer,\n",
    "                      n_epochs=10, output_activation=None, metrics=None):\n",
    "    n_inputs = X_train.shape[-1]\n",
    "    encoder = keras.models.Sequential([\n",
    "        keras.layers.Dense(n_neurons, activation=\"selu\", input_shape=[n_inputs])\n",
    "    ])\n",
    "    decoder = keras.models.Sequential([\n",
    "        keras.layers.Dense(n_inputs, activation=output_activation),\n",
    "    ])\n",
    "    autoencoder = keras.models.Sequential([encoder, decoder])\n",
    "    autoencoder.compile(optimizer, loss, metrics=metrics)\n",
    "    autoencoder.fit(X_train, X_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid, X_valid))\n",
    "    return encoder, decoder, encoder(X_train), encoder(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "K = keras.backend\n",
    "X_train_flat = K.batch_flatten(X_train) # equivalent to .reshape(-1, 28 * 28)\n",
    "X_valid_flat = K.batch_flatten(X_valid)\n",
    "enc1, dec1, X_train_enc1, X_valid_enc1 = train_autoencoder(\n",
    "    100, X_train_flat, X_valid_flat, \"binary_crossentropy\",\n",
    "    keras.optimizers.SGD(learning_rate=1.5), output_activation=\"sigmoid\",\n",
    "    metrics=[rounded_accuracy])\n",
    "enc2, dec2, _, _ = train_autoencoder(\n",
    "    30, X_train_enc1, X_valid_enc1, \"mse\", keras.optimizers.SGD(learning_rate=0.05),\n",
    "    output_activation=\"selu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ae_1_by_1 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    enc1, enc2, dec2, dec1,\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(stacked_ae_1_by_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ae_1_by_1.compile(loss=\"binary_crossentropy\",\n",
    "                          optimizer=keras.optimizers.SGD(learning_rate=0.1), metrics=[rounded_accuracy])\n",
    "history = stacked_ae_1_by_1.fit(X_train, X_train, epochs=10,\n",
    "                                validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(stacked_ae_1_by_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 오토인코더"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순환 오토인코더"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잡음 제거 오토인코더"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 희소 오토인코더"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변이형 오토인코더"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 패션 MNIST 이미지 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 생성적 적대 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **생성자**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **판별자**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAN 훈련의 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 심층 합성곱 GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ProGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **미니배치 표준편차 층**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **동일한 학습 속도**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **픽셀별 정규화 층**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StyleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **합성 네트워크**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "813af78ecda15588a7e82817c6b6453ec390e9c163778a4ec46b9b973fd11dd3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf_pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
